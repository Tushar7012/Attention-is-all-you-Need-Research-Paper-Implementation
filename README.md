# ğŸ“š Attention Is All You Need â€” Transformer (TensorFlow)

This project is a **full implementation** of the famous [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) paper (**Vaswani et al., 2017**) using **TensorFlow 2.x**, built **from scratch**.

It covers:
- Encoder & Decoder layers
- Multi-Head Scaled Dot-Product Attention
- Positional Encoding
- Custom Learning Rate Schedule (Noam)
- Teacher Forcing training loop
- BLEU score evaluation & inference

---

## ğŸ“Œ **Dataset**

This demo uses **Portuguese â†’ English** translation from the WMT dataset.  
Example test input:

---

## ğŸ“‚ **Project Structure**
  Attention-Is-All-You-Need/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ layers.py # Attention, FFN, Positional Encoding
â”‚ â”œâ”€â”€ model.py # Full Transformer architecture
â”‚ â”œâ”€â”€ utils.py # Masks, BLEU, helper functions
â”‚ â”œâ”€â”€ dataset.py # Tokenizer, tf.data input pipeline
â”‚ â”œâ”€â”€ train.py # Training loop (saves weights)
â”‚ â”œâ”€â”€ inference.py # Loads weights & runs translation


---

## âš™ï¸ **Installation**

```bash
python -m venv venv
venv\Scripts\activate      

pip install -r requirements.txt

